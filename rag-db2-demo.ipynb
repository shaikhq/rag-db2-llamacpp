{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38184a03",
   "metadata": {},
   "source": [
    "### Local RAG Workflow with Db2 and llama.cpp\n",
    "\n",
    "This notebook demonstrates a complete Retrieval-Augmented Generation (RAG) pipeline running on a local system:\n",
    "\n",
    "* **Embeddings** are generated locally using the Granite model served via `llama.cpp`\n",
    "* **Vector search** is performed in Db2 using built-in `VECTOR` functions\n",
    "* **Context retrieval** and **prompt construction** are handled locally\n",
    "* Only the **final text generation** step uses a hosted LLM (Mistral) via Watsonx.ai\n",
    "\n",
    "By keeping embedding generation and prompting local, the workflow reduces latency, avoids cloud dependency for sensitive data, and offers more control over the overall process.\n",
    "\n",
    "**Setup and Requirements**\n",
    "For installation steps and additional context, see the accompanying [README.md](./README.md).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f5ff2",
   "metadata": {},
   "source": [
    "## Step 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8a40ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1708: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2305: SyntaxWarning: invalid escape sequence '\\?'\n",
      "/tmp/ipykernel_389282/2299624180.py:1708: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  firstCommand = \"(?:^\\s*)([a-zA-Z]+)(?:\\s+.*|$)\"\n",
      "/tmp/ipykernel_389282/2299624180.py:2305: SyntaxWarning: invalid escape sequence '\\?'\n",
      "  pattern = \"\\?\\*[0-9]+\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Db2 Extensions Loaded. Version: 2024-09-16\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "# NLP and embeddings\n",
    "import spacy\n",
    "import trafilatura\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Watsonx AI\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams, EmbedTextParamsMetaNames\n",
    "\n",
    "# LangChain components\n",
    "from langchain_ibm import WatsonxLLM\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Notebook display utilities\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "if not os.path.isfile('db2.ipynb'):\n",
    "    os.system('wget https://raw.githubusercontent.com/IBM/db2-jupyter/master/db2.ipynb')\n",
    "\n",
    "%run db2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342720d9",
   "metadata": {},
   "source": [
    "## Step 1: Web Content Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ef41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despite being one of the earlier machine learning techniques, linear regression continues to be a top choice among ML practitioners for a regression task. For the past three years, over 80% of the respondents to Kaggle’s annual state of data science and machine learning survey mentioned linear regression as a ML algorithm they most frequently use. IBM Db2 provides an in-database stored procedure (SP) for Linear Regression as part of its ML library, which is a collection of over 200 SPs for performing different ML tasks in the database. Using the linear regression SP and other functionality of DB2’s ML Library, ML practitioners can build and deploy linear regression models in the database when their ML dataset is available in a Db2 database. In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\n",
      "Let’s begin.\n",
      "The Regression Task\n",
      "In this exercise, I will use the GoSales dataset, which is available from this link.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://community.ibm.com/community/user/blogs/shaikh-quader/2024/05/07/building-an-in-db-linear-regression-model-with-ibm'\n",
    "downloaded = trafilatura.fetch_url(url)\n",
    "\n",
    "if downloaded:\n",
    "    article = trafilatura.extract(downloaded)\n",
    "    print(article[:1000])  # Preview first 1000 chars\n",
    "else:\n",
    "    print(\"Failed to fetch content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53037a08",
   "metadata": {},
   "source": [
    "## Step 2: Chunking the Clean Text for Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6353970",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76455699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_sentence_chunker(text, max_words=200, overlap_words=50):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        sentence = sentences[i]\n",
    "        sentence_length = len(sentence.split())\n",
    "\n",
    "        if current_length + sentence_length <= max_words:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "            i += 1\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            # Start new chunk with overlap\n",
    "            overlap = []\n",
    "            overlap_len = 0\n",
    "            j = len(current_chunk) - 1\n",
    "            while j >= 0 and overlap_len < overlap_words:\n",
    "                s = current_chunk[j]\n",
    "                overlap.insert(0, s)\n",
    "                overlap_len += len(s.split())\n",
    "                j -= 1\n",
    "            current_chunk = overlap\n",
    "            current_length = overlap_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757864ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 chunks created.\n",
      "Despite being one of the earlier machine learning techniques, linear regression continues to be a top choice among ML practitioners for a regression task. For the past three years, over 80% of the respondents to Kaggle’s annual state of data science and machine learning survey mentioned linear regression as a ML algorithm they most frequently use. IBM Db2 provides an in-database stored procedure (SP) for Linear Regression as part of its ML library, which is a collection of over 200 SPs for performing different ML tasks in the database. Using the linear regression SP and other functionality of DB2’s ML Library, ML practitioners can build and deploy linear regression models in the database when their ML dataset is available in a Db2 database. In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\n",
      "Let’s begin. The Regression Task\n",
      "In this exercise, I will use the GoSales dataset, which is available from this link. The dataset has 60252 synthetic customers’ profile and their purchase amount at an imaginary outdoor equipment store.\n"
     ]
    }
   ],
   "source": [
    "chunks = overlapping_sentence_chunker(article, max_words=200, overlap_words=50)\n",
    "print(f\"{len(chunks)} chunks created.\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270abedb",
   "metadata": {},
   "source": [
    "## Generating Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5823e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Llama(model_path=\"models/granite-embedding-30m-english-Q6_K.gguf\", embedding=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a43b6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_model.create_embedding(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e010689f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Despite being one of the earlier machine learning techniques, linear regression continues to be a top choice among ML practitioners for a regression task. For the past three years, over 80% of the respondents to Kaggle’s annual state of data science and machine learning survey mentioned linear regression as a ML algorithm they most frequently use. IBM Db2 provides an in-database stored procedure (SP) for Linear Regression as part of its ML library, which is a collection of over 200 SPs for performing different ML tasks in the database. Using the linear regression SP and other functionality of DB2’s ML Library, ML practitioners can build and deploy linear regression models in the database when their ML dataset is available in a Db2 database. In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\n",
      "Let’s begin. The Regression Task\n",
      "In this exercise, I will use the GoSales dataset, which is available from this link. The dataset has 60252 synthetic customers’ profile and their purchase amount at an imaginary outdoor equipment store.\n",
      "Embedding (partial): [-0.2705141305923462, -0.29837024211883545, 3.617060899734497, -1.5401177406311035, 3.0441579818725586, -0.6652486324310303, -1.4385193586349487, -0.38645970821380615, 1.7064034938812256, 2.6135401725769043, -0.4143952429294586, -0.9539514183998108] ...\n",
      "\n",
      "Text: In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\n",
      "Let’s begin. The Regression Task\n",
      "In this exercise, I will use the GoSales dataset, which is available from this link. The dataset has 60252 synthetic customers’ profile and their purchase amount at an imaginary outdoor equipment store. The following is the list of columns in the dataset:\n",
      "I want to learn a multiple linear regression function of the following equation form that will use as input the first four columns — AGE, MARITAL_STATUS, and PROFESSION — and predict the PURCHASE_AMOUNT. Using the training examples, the linear regression algorithm will learn the values of the following five parameters — the first one is the intercept and the remaining are four coefficients, one per input column. Following these steps, I created a Db2 table with the name GOSALES_FULL under GOSALES schema and loaded the dataset into it. Train / Test Split\n",
      "First, I will divide the records from the GOSALES_FULL table into two partitions: a training partition and a test partition. For making these partitions, I will call the SPLIT_DATA stored procedure (SP) as follows.\n",
      "Embedding (partial): [-0.6316127181053162, -1.0422494411468506, 1.3213083744049072, -1.6151585578918457, 1.7298030853271484, -1.1513350009918213, -1.0865113735198975, -1.0979628562927246, 2.40102481842041, 1.6998119354248047, 1.4956462383270264, -0.6277296543121338] ...\n",
      "\n",
      "Text: Following these steps, I created a Db2 table with the name GOSALES_FULL under GOSALES schema and loaded the dataset into it. Train / Test Split\n",
      "First, I will divide the records from the GOSALES_FULL table into two partitions: a training partition and a test partition. For making these partitions, I will call the SPLIT_DATA stored procedure (SP) as follows. CALL IDAX.SPLIT_DATA('intable=GOSALES.GOSALES_FULL, id=ID, traintable=GOSALES.GOSALES_TRAIN, testtable=GOSALES.GOSALES_TEST, fraction=0.8, seed=1') These are the parameters I passed to this SP: (1) intable: the input table from which I want to create partitions. The value is GOSALES_FULL; (2) id: the name of the id column in the input table; (3) traintable: the name I want to give to the table that will have the training records; (4) testtable: the name of the output table where the SP will store the test records; (5) fraction: the portion of the records that I want in the training partition; (6) seed: I can set a value for reproducibility of the same partitions. The SP will randomly divide records from the GOSALES_FULL table into two output tables: GOSALES_TRAIN and GOSALES_TEST. The following two SQL statements will count the number of records in these two tables.\n",
      "Embedding (partial): [-1.1674954891204834, 0.651764988899231, 0.688299834728241, -1.5352256298065186, 1.4445104598999023, 0.46495747566223145, -1.995290756225586, -2.0386345386505127, 3.3424839973449707, -1.0567870140075684, 1.9267780780792236, -1.0037881135940552] ...\n",
      "\n",
      "Text: The value is GOSALES_FULL; (2) id: the name of the id column in the input table; (3) traintable: the name I want to give to the table that will have the training records; (4) testtable: the name of the output table where the SP will store the test records; (5) fraction: the portion of the records that I want in the training partition; (6) seed: I can set a value for reproducibility of the same partitions. The SP will randomly divide records from the GOSALES_FULL table into two output tables: GOSALES_TRAIN and GOSALES_TEST. The following two SQL statements will count the number of records in these two tables. SELECT count(*) FROM GOSALES.GOSALES_TRAIN\n",
      "SELECT count(*) FROM GOSALES.GOSALES_TEST\n",
      "The above counts confirm that the train and test tables have 80% and 20% of records, respectively, from the original table with 60252 records. Data Exploration\n",
      "Now, I will look into some sample records from the training dataset, GOSALES_TRAIN. SELECT * FROM GOSALES.GOSALES_TRAIN FETCH FIRST 5 ROWS ONLY\n",
      "From the above sample records, I get a feel for the customer records I will work with. Next, I will generate summary statistics of the entire training set using SUMMARY1000 SP:\n",
      "CALL IDAX.SUMMARY1000('intable=GOSALES.GOSALES_TRAIN, outtable=GOSALES.GOSALES_TRAIN_SUM1000, incolumn=GENDER;AGE;MARITAL_STATUS;PROFESSION')\n",
      "Embedding (partial): [0.5690323114395142, -0.8686362504959106, 1.738566517829895, 0.3102778196334839, 0.34752774238586426, -0.08756399154663086, -1.8359187841415405, -0.5431548357009888, 1.2985225915908813, -1.1961960792541504, 0.8922017812728882, -0.6628283858299255] ...\n",
      "\n",
      "Text: Data Exploration\n",
      "Now, I will look into some sample records from the training dataset, GOSALES_TRAIN. SELECT * FROM GOSALES.GOSALES_TRAIN FETCH FIRST 5 ROWS ONLY\n",
      "From the above sample records, I get a feel for the customer records I will work with. Next, I will generate summary statistics of the entire training set using SUMMARY1000 SP:\n",
      "CALL IDAX.SUMMARY1000('intable=GOSALES.GOSALES_TRAIN, outtable=GOSALES.GOSALES_TRAIN_SUM1000, incolumn=GENDER;AGE;MARITAL_STATUS;PROFESSION') In this SP call, intable parameter has the name of the training table from which I wanted to gather summary statistics. outtable parameter has the name of the output table where I want the SP to save the summary statistics of the entire training table. In the incolumn parameter, I list the columns whose statistics I want to collect. I am calling this SP with these parameters: (1) intable: name of the table whose sumary statistics I want to gather, the training partition in this case; (2) outtable: name of the table where I want the SP to store the overall summary statistics, (3) incolumn: list of columns whose statistics I want to collect.\n",
      "Embedding (partial): [1.724369764328003, 0.5899183750152588, 1.9859778881072998, 1.470940113067627, -0.6918668746948242, 0.0038404464721679688, -2.051774501800537, -2.65948748588562, -0.14065110683441162, -1.8984001874923706, 0.0734562873840332, 1.5980373620986938] ...\n",
      "\n",
      "Text: I am calling this SP with these parameters: (1) intable: name of the table whose sumary statistics I want to gather, the training partition in this case; (2) outtable: name of the table where I want the SP to store the overall summary statistics, (3) incolumn: list of columns whose statistics I want to collect. SUMMARY1000 SP saves the collected statistics from the training table in three output tables: (1) GOSALES_TRAIN_SUM1000: has summary statistics of all columns listed in the incolumn parameter; (2) GOSALES_TRAIN_SUM1000_NUM: has summary statistics of only the numeric columns from the incolumn parameter, (3) GOSALES_TRAIN_SUM1000_CHAR: has summary statistics of categorical columns from the incolumn parameter. For the ease of viewing, I will look at the summary statistics of each column type separately — first the numeric type and then nominal type. SELECT * FROM GOSALES.GOSALES_TRAIN_SUM1000_NUM The dataset has one numeric feature column, AGE. This summary table has provides a range of statistics, such as mean, variance, skewness. Also, the table reports that the AGE column has 1878 missing values. Similarly, I find the following summary statistics from the GOSALES.GOSALES_TRAIN_SUM1000_CHAR table.\n",
      "Embedding (partial): [1.8275442123413086, 0.1317017674446106, 3.656879425048828, 1.6801018714904785, -0.8150551915168762, -1.791543960571289, -0.6924350261688232, -3.982452154159546, -0.9168041944503784, -1.577769160270691, -0.7550859451293945, 1.0993341207504272] ...\n",
      "\n",
      "Text: For the ease of viewing, I will look at the summary statistics of each column type separately — first the numeric type and then nominal type. SELECT * FROM GOSALES.GOSALES_TRAIN_SUM1000_NUM The dataset has one numeric feature column, AGE. This summary table has provides a range of statistics, such as mean, variance, skewness. Also, the table reports that the AGE column has 1878 missing values. Similarly, I find the following summary statistics from the GOSALES.GOSALES_TRAIN_SUM1000_CHAR table. SELECT * FROM GOSALES.GOSALES_TRAIN_SUM1000_CHAR\n",
      "From this summary, I can see that all three nominal columns have some missing values. Data Preprocessing\n",
      "From data exploration, I identified two kinds of data preprocessing tasks: handling missing values and dealing with nominal columns. In this step, I will address both tasks. Dealing with missing values\n",
      "I will replace missing values in the training dataset using IMPUTE_DATA SP. This SP supports replacing missing values with one of the four methods: mean, median, most frequent value, or a constant. All four methods work with numeric columns, whereas only the last two methods apply to nominal columns. AGE column: I will replace missing values in the AGE column with the mean age value:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, incolumn=AGE, method=mean')\n",
      "Embedding (partial): [1.9391002655029297, -0.040575623512268066, 3.0040030479431152, 0.9745398759841919, 2.261876106262207, -1.3090643882751465, 0.07904928922653198, -0.6328096389770508, -1.497769832611084, -0.9337043762207031, 0.10596895217895508, -0.15925414860248566] ...\n",
      "\n",
      "Text: This SP supports replacing missing values with one of the four methods: mean, median, most frequent value, or a constant. All four methods work with numeric columns, whereas only the last two methods apply to nominal columns. AGE column: I will replace missing values in the AGE column with the mean age value:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, incolumn=AGE, method=mean') In the above SP call, I passed three parameters: (1) intable: the name of the input table, which is GOSALES_TRAIN, (2) incolumn: the name of the column for imputing missing values. AGE is the column name. (3) method: a supported imputation strategy. I chose mean.) Similarly, the following SP calls will replace missing values in GENDER, MARITAL_STATUS, and PROFESSION columns with the most frequent value of each column. I looked up the most frequent values of each column from the GOSALES_TRAIN_SUM1000_CHAR table that was created during the data exploration step. Imputing missing values in the GENDER column:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, method=replace, nominalValue=M, incolumn=GENDER') Imputing missing values in the MARITAL_STATUS column:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, method=replace, nominalValue=Married, incolumn=MARITAL_STATUS')\n",
      "Embedding (partial): [0.22937703132629395, 3.393791675567627, 4.02763032913208, 0.3009425103664398, 0.002567112445831299, -0.5185493230819702, -1.054365634918213, -2.6164681911468506, 1.4903947114944458, -0.2640853524208069, -1.2857129573822021, 0.6674286127090454] ...\n",
      "\n",
      "Text: Similarly, the following SP calls will replace missing values in GENDER, MARITAL_STATUS, and PROFESSION columns with the most frequent value of each column. I looked up the most frequent values of each column from the GOSALES_TRAIN_SUM1000_CHAR table that was created during the data exploration step. Imputing missing values in the GENDER column:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, method=replace, nominalValue=M, incolumn=GENDER') Imputing missing values in the MARITAL_STATUS column:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, method=replace, nominalValue=Married, incolumn=MARITAL_STATUS') Imputing missing values in the PROFESSION column:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TRAIN, method=replace, nominalValue=Other, incolumn=PROFESSION')\n",
      "By running the following SQL statement, I can confirm that the AGE column in the training dataset is now free of any missing values:\n",
      "SELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE AGE IS NULL\n",
      "Similarly, the following statements will count missing values in GENDER, MARITAL_STATUS, and PROFESSION columns. All of them no longer have any missing value. Count missing values in the GENDER COLUMN:\n",
      "SELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE GENDER IS NULL\n",
      "Count missing values in the MARITAL_STATUS column:\n",
      "SELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE MARITAL_STATUS IS NULL\n",
      "Count missing values in the PROFESSION column:\n",
      "SELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE PROFESSION IS NULL\n",
      "Dealing with nominal columns\n",
      "Linear regression algorithm requires that all its input columns are numeric.\n",
      "Embedding (partial): [1.4102089405059814, 3.5528411865234375, 4.500090599060059, 1.3052136898040771, 0.38854992389678955, -0.2343451976776123, -0.4038812220096588, -0.6452935338020325, -0.193708598613739, 0.2892957925796509, -1.958526611328125, -0.7278369069099426] ...\n",
      "\n",
      "Text: Count missing values in the GENDER COLUMN:\n",
      "SELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE GENDER IS NULL\n",
      "Count missing values in the MARITAL_STATUS column:\n",
      "SELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE MARITAL_STATUS IS NULL\n",
      "Count missing values in the PROFESSION column:\n",
      "SELECT count(*) FROM GOSALES.GOSALES_TRAIN WHERE PROFESSION IS NULL\n",
      "Dealing with nominal columns\n",
      "Linear regression algorithm requires that all its input columns are numeric. So, before invoking a linear regression algorithm, ML practitioners convert any non-numeric columns to numbers following some encoding scheme — such as 1-hot encoding. The linear regression SP of Db2 can natively handle nominal columns. When the input table has any nominal column, the SP internally converts it into a set of numeric columns using 1-hot encoding. So, I can leave the three nominal columns — GENDER, MARITAL_STATUS, and PROFESSION — as-is and let the SP take care of them. Now, the GOSALES_TRAIN dataset is ready for model training. Model Training\n",
      "The following call to the LINEAR_REGRESSION SP will begin training of a linear regression model using training examples from the GOSALES_TRAIN table. The SP will use the list of columns mentioned in the incolum paramter as input features and the column from the target parameter as the output column.\n",
      "Embedding (partial): [1.4032692909240723, -0.04158550500869751, 4.619534015655518, -0.23295611143112183, 2.417262554168701, -1.371084213256836, -0.3849913477897644, -0.7475572228431702, -0.25856471061706543, -0.3691386878490448, -0.9595062732696533, -0.034464627504348755] ...\n",
      "\n",
      "Text: Model Training\n",
      "The following call to the LINEAR_REGRESSION SP will begin training of a linear regression model using training examples from the GOSALES_TRAIN table. The SP will use the list of columns mentioned in the incolum paramter as input features and the column from the target parameter as the output column. Since the intercept parameter is set to true, the SP will learn the value of intercept. CALL IDAX.LINEAR_REGRESSION('model=GOSALES.GOSALES_LINREG, intable=GOSALES.GOSALES_TRAIN, id=ID, target=PURCHASE_AMOUNT,incolumn=AGE;GENDER;MARITAL_STATUS;PROFESSION, intercept=true') After the training completes, the SP will add the new model, GOSALES_LINREG, to Db2’s model catalog. The following SP call will show the list of existing models in the catalog, which now includes my new model. CALL IDAX.LIST_MODELS('format=short, all=true')\n",
      "Additionally, the LINEAR_REGRESSION SP saves the learned values of intercept and the coefficients, along with several other learned parameter values, in a table. The table’s name takes this form: MODELNAME_MODEL. For the GOSALES_LINREG model, the name of its metadata table is GOSALES_LINREG_MODEL. The following SQL will display the values of the learned model parameters. SELECT VAR_NAME, LEVEL_NAME, VALUE FROM GOSALES.GOSALES_LINREG_MODEL You may have noticed that the above output has more feature columns than what I originally had in the training set.\n",
      "Embedding (partial): [-1.3670271635055542, 0.22973984479904175, 3.918409824371338, -2.400296211242676, 0.9902483224868774, -0.7532920837402344, 0.2996814250946045, -0.9454855918884277, 2.3948488235473633, 0.4055037498474121, -0.04382789134979248, -0.8120542764663696] ...\n",
      "\n",
      "Text: For the GOSALES_LINREG model, the name of its metadata table is GOSALES_LINREG_MODEL. The following SQL will display the values of the learned model parameters. SELECT VAR_NAME, LEVEL_NAME, VALUE FROM GOSALES.GOSALES_LINREG_MODEL You may have noticed that the above output has more feature columns than what I originally had in the training set. Because LINEAR_REGRESSION SP has applied 1-hot encoding on the nominal columns, the final feature set has one feature for each distinct value in the nominal columns. Generating Predictions with the Model In this step, I will use the GOSALES_LINREG model to predict the purchase amount of the customers in the GOSALE_TEST table. Before generating predictions, I will preprocess this dataset using the preprocessing steps I had used with the training dataset, which were imputing missing values. I will use the following SQL statements to impute missing values in AGE, GENDER, MARITAL_STATUS, and PROFESSION columns of the test dataset. Imputing missing values in the AGE column:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=mean, incolumn=AGE') Imputing missing values in the GENDER column:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=replace, nominalValue=M, incolumn=GENDER')\n",
      "Imputing missing values in the MARITAL_STATUS column:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=replace, nominalValue=Married, incolumn=MARITAL_STATUS')\n",
      "Embedding (partial): [1.7691915035247803, 0.9187219142913818, 4.077812194824219, 1.307729959487915, 0.001743018627166748, -1.551095962524414, 1.5203049182891846, -1.0888124704360962, -1.4882519245147705, -0.26697850227355957, -1.3621149063110352, 0.5872418880462646] ...\n",
      "\n",
      "Text: I will use the following SQL statements to impute missing values in AGE, GENDER, MARITAL_STATUS, and PROFESSION columns of the test dataset. Imputing missing values in the AGE column:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=mean, incolumn=AGE') Imputing missing values in the GENDER column:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=replace, nominalValue=M, incolumn=GENDER')\n",
      "Imputing missing values in the MARITAL_STATUS column:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=replace, nominalValue=Married, incolumn=MARITAL_STATUS') Imputing missing values in the PROFESSION column:\n",
      "CALL IDAX.IMPUTE_DATA('intable=GOSALES.GOSALES_TEST, method=replace, nominalValue=Other, incolumn=PROFESSION')\n",
      "Now the test dataset is ready to be passed as input to the model. PREDICT_LINEAR_REGRESSION SP will use GOSALES_LINREG model to generate predictions for records in the GOSALES_TEST table. The SP will save the predictions in the GOSALES_TEST_PREDICTIONS table, as per the table name specified in the outtable parameter. CALL IDAX.PREDICT_LINEAR_REGRESSION('model=GOSALES.GOSALES_LINREG, intable=GOSALES.GOSALES_TEST, outtable=GOSALES.GOSALES_TEST_PREDICTIONS, id=ID') The following SQL displays sample predictions from the GOSALES_TEST_PREDICTIONS table. SELECT * FROM GOSALES.GOSALES_TEST_PREDICTIONS FETCH FIRST 5 ROWS ONLY\n",
      "Model Evaluation\n",
      "For the test customer records, the actual purchase price is available at the GOSALES_TEST table and the predicted purchase price is in the GOSALES_TEST_PREDICTIONS table. Based on these actual and the predicted purchase amounts, I can now compute Mean Squared Error (MSE), Mean Absolute Error (MAE), and Mean Absolute Percent Error (MAPE) via SQL.\n",
      "Embedding (partial): [0.43076562881469727, 0.9662498831748962, 4.157407760620117, 1.9825021028518677, -2.809661865234375, 0.41553735733032227, 2.253106117248535, -0.9911311864852905, -1.0045251846313477, -0.23363180458545685, -1.7631266117095947, 1.71903395652771] ...\n",
      "\n",
      "Text: SELECT * FROM GOSALES.GOSALES_TEST_PREDICTIONS FETCH FIRST 5 ROWS ONLY\n",
      "Model Evaluation\n",
      "For the test customer records, the actual purchase price is available at the GOSALES_TEST table and the predicted purchase price is in the GOSALES_TEST_PREDICTIONS table. Based on these actual and the predicted purchase amounts, I can now compute Mean Squared Error (MSE), Mean Absolute Error (MAE), and Mean Absolute Percent Error (MAPE) via SQL. This will help me evaluate the model’s predictive performance. MSE:\n",
      "I will use MSE SP to compute the MSE:\n",
      "CALL IDAX.MSE('intable=GOSALES.GOSALES_TEST, id=ID, target=PURCHASE_AMOUNT, resulttable=GOSALES.GOSALES_TEST_PREDICTIONS, resulttarget=PURCHASE_AMOUNT')\n",
      "MAE:\n",
      "I will use MAE SP to calculate MAE:\n",
      "CALL IDAX.MAE('intable=GOSALES.GOSALES_TEST, id=ID, target=PURCHASE_AMOUNT, resulttable=GOSALES.GOSALES_TEST_PREDICTIONS, resulttarget=PURCHASE_AMOUNT')\n",
      "MAPE: I wrote the following SQL to compute MAPE: SELECT avg(abs(A.PURCHASE_AMOUNT - B.PURCHASE_AMOUNT) / A.PURCHASE_AMOUNT * 100) AS MAPE FROM GOSALES.GOSALES_TEST AS A, GOSALES.GOSALES_TEST_PREDICTIONS AS B WHERE A.ID = B.ID\n",
      "Dropping the Model\n",
      "If I want to drop this model, I can use the DROP_MODEL SP:\n",
      "CALL IDAX.DROP_MODEL('model=GOSALES.GOSALES_LINREG')\n",
      "Conclusion\n",
      "In this exercise, I built, evaluated, and deployed an end-to-end linear regression pipeline using 29 simple SQL queries — 17 of them are calls to SPs provided by Db2, and the remaining ones were SELECT statements.\n",
      "Embedding (partial): [-0.7395018935203552, 2.0542449951171875, 0.35278937220573425, -1.017642617225647, -0.1261531114578247, -2.8704302310943604, 0.8163844347000122, -1.738744854927063, -0.19532275199890137, 1.0674240589141846, -1.020693302154541, 0.5626742839813232] ...\n",
      "\n",
      "Text: A.PURCHASE_AMOUNT * 100) AS MAPE FROM GOSALES.GOSALES_TEST AS A, GOSALES.GOSALES_TEST_PREDICTIONS AS B WHERE A.ID = B.ID\n",
      "Dropping the Model\n",
      "If I want to drop this model, I can use the DROP_MODEL SP:\n",
      "CALL IDAX.DROP_MODEL('model=GOSALES.GOSALES_LINREG')\n",
      "Conclusion\n",
      "In this exercise, I built, evaluated, and deployed an end-to-end linear regression pipeline using 29 simple SQL queries — 17 of them are calls to SPs provided by Db2, and the remaining ones were SELECT statements. In this ML workflow, I didn’t need to bring any data outside of the database, nor did I need separate infrastructure for developing, training, and serving ML models. For many companies, machine learning in the database can be a cost effective and quicker path to embrace AI.\n",
      "Related Resources\n",
      "For creating an in-database ML pipeline using my above steps, you can download the GoSales dataset from this link and follow instructions on this page to set up a Db2 database. To learn more about Db2’s in-database ML library, check out Db2 production documentation. To find more examples of creating in-database ML models with Db2, check out this GitHub repo. #Featured-area-2\n",
      "Embedding (partial): [-0.38895756006240845, 0.8255919218063354, 2.397259473800659, -1.1804941892623901, 1.6013076305389404, -0.5516656041145325, -0.7267894148826599, -1.2757048606872559, 2.646341323852539, 1.4569473266601562, -2.001006603240967, 0.8798753023147583] ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text, item in zip(chunks, embeddings[\"data\"]):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Embedding (partial):\", item[\"embedding\"][:12], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d707a81",
   "metadata": {},
   "source": [
    "## Inserting Vectors into Db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea52ddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful. SAMPLE @ localhost \n",
      "Command completed.\n",
      "Command completed.\n"
     ]
    }
   ],
   "source": [
    "db2creds = dotenv_values('.env')\n",
    "%sql CONNECT CREDENTIALS db2creds\n",
    "\n",
    "%sql DROP TABLE IF EXISTS embeddings\n",
    "\n",
    "SQL_CREATE_TABLE = \"\"\"\n",
    "CREATE TABLE embeddings (\n",
    "    id INT NOT NULL GENERATED ALWAYS AS IDENTITY \n",
    "        (START WITH 1, INCREMENT BY 1),\n",
    "    content CLOB,\n",
    "    embedding VECTOR(384, FLOAT32),\n",
    "    PRIMARY KEY (id)\n",
    ")\n",
    "\"\"\"\n",
    "%sql {SQL_CREATE_TABLE}\n",
    "\n",
    "# Prepare values: list of tuples (content, embedding_vector_str)\n",
    "values = []\n",
    "for i, (text, item) in enumerate(zip(chunks, embeddings[\"data\"])):\n",
    "    embedding = item.get(\"embedding\")\n",
    "    if embedding and len(embedding) == 384:\n",
    "        vector_str = \"[\" + \", \".join(map(str, embedding)) + \"]\"\n",
    "        values.append((text, vector_str))\n",
    "    else:\n",
    "        print(f\"Skipping row {i+1}: invalid embedding\")\n",
    "\n",
    "# Prepare SQL statement with VECTOR function\n",
    "stmt = %sql prepare INSERT INTO embeddings (content, embedding) VALUES (?, VECTOR(?, 384, FLOAT32))\n",
    "\n",
    "# Disable autocommit\n",
    "%sql autocommit off\n",
    "\n",
    "# Execute prepared insert statement using magic-style loop\n",
    "for content, vector_str in values:\n",
    "    %sql execute :stmt using :content, :vector_str\n",
    "\n",
    "# Commit the work\n",
    "%sql commit work\n",
    "\n",
    "# Enable autocommit back\n",
    "%sql autocommit on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b2000",
   "metadata": {},
   "source": [
    "# Ask LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ddc4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to build a linear regression model using IDAX?\"\n",
    "# question = 'How to see the list of in database ML models in Db2?'\n",
    "# question = 'How to impute missing values of columns in Db2?'\n",
    "# query = 'What is Python UDF?'\n",
    "\n",
    "embedding = embedding_model.create_embedding(question)\n",
    "query_vector = embedding['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b31b2",
   "metadata": {},
   "source": [
    "## Vector Search and Context Retrieval (Db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d642f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Format query embedding for Db2\n",
    "query_embedding_str = '[' + ', '.join(map(str, query_vector)) + ']'\n",
    "top_k = 5  # You can adjust this number\n",
    "\n",
    "# Step 2: SQL to get top-k most similar chunks based on Euclidean distance\n",
    "SQL_DISTANCE = f\"\"\"\n",
    "SELECT \n",
    "    content AS CONTEXT,\n",
    "    VECTOR_DISTANCE(\n",
    "        VECTOR('{query_embedding_str}', 384, FLOAT32),\n",
    "        embedding,\n",
    "        EUCLIDEAN\n",
    "    ) AS DISTANCE\n",
    "FROM embeddings\n",
    "ORDER BY DISTANCE ASC\n",
    "FETCH FIRST {top_k} ROWS ONLY\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Run the query with db2magic and store the result in a DataFrame\n",
    "result_df = %sql {SQL_DISTANCE}\n",
    "\n",
    "# Step 4: Combine all top-k text chunks into a single context string\n",
    "context_str = \"\\n\\n\".join(result_df[\"CONTEXT\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826dc32",
   "metadata": {},
   "source": [
    "## LLM setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd12400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "load_dotenv(os.getcwd()+\"/.env\", override=True)\n",
    "\n",
    "# llm for generating responses\n",
    "def get_llm():\n",
    "    # model_id = 'meta-llama/llama-3-1-70b-instruct'\n",
    "    model_id = 'mistralai/mistral-large'\n",
    "    parameters = {\n",
    "        GenParams.MAX_NEW_TOKENS: 512,\n",
    "        GenParams.TEMPERATURE: 0.6,\n",
    "    }\n",
    "   \n",
    "    watsonx_llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        apikey=os.getenv(\"WATSONX_APIKEY\", \"\"),\n",
    "        project_id=os.getenv(\"WATSONX_PROJECT\", \"\"),\n",
    "        params=parameters,\n",
    "    )\n",
    "    return watsonx_llm\n",
    "    \n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d5da3",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bc8beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a knowledgeable assistant. Answer the question based solely on the provided context.\n",
    "- If the context contains the answer, respond directly to the reader using 'you' to make it personal.\n",
    "- If the answer includes code, provide an explanation of the code following the code block.\n",
    "- If the information is not available in the context, respond with 'The information is not available in the provided context.'\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1add39",
   "metadata": {},
   "source": [
    "### Generate an Answer Using wx.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "910d27e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Q:** How to build a linear regression model using IDAX?\n",
       "\n",
       "**A:** \n",
       "To build a linear regression model using IDAX, you can use the `LINEAR_REGRESSION` stored procedure (SP) provided by IBM Db2. Here is the SQL call to train a linear regression model:\n",
       "\n",
       "```sql\n",
       "CALL IDAX.LINEAR_REGRESSION('model=GOSALES.GOSALES_LINREG, intable=GOSALES.GOSALES_TRAIN, id=ID, target=PURCHASE_AMOUNT, incolumn=AGE;GENDER;MARITAL_STATUS;PROFESSION, intercept=true');\n",
       "```\n",
       "\n",
       "This code specifies the parameters for the linear regression model, including the table containing the training data (`intable=GOSALES.GOSALES_TRAIN`), the target column (`target=PURCHASE_AMOUNT`), and the input features (`incolumn=AGE;GENDER;MARITAL_STATUS;PROFESSION`). The `intercept=true` parameter means that the model will learn the value of the intercept.\n",
       "\n",
       "After the training completes, the new model `GOSALES_LINREG` will be added to Db2’s model catalog. You can list the existing models in the catalog with the following call:\n",
       "\n",
       "```sql\n",
       "CALL IDAX.LIST_MODELS('format=short, all=true');\n",
       "```\n",
       "\n",
       "Additionally, the learned values of the intercept and the coefficients, along with other learned parameter values, are saved in a table named `GOSALES_LINREG_MODEL`. You can display these values using the following SQL query:\n",
       "\n",
       "```sql\n",
       "SELECT VAR_NAME, LEVEL_NAME, VALUE FROM GOSALES.GOSALES_LINREG_MODEL;\n",
       "```\n",
       "\n",
       "This process allows you to build and manage a linear regression model directly within the Db2 database."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Custom prompt template (no LangChain needed)\n",
    "prompt_template = \"\"\"\n",
    "You are a knowledgeable assistant. Answer the question based solely on the provided context.\n",
    "- If the context contains the answer, respond directly to the reader using 'you' to make it personal.\n",
    "- If the answer includes code, provide an explanation of the code following the code block.\n",
    "- If the information is not available in the context, respond with 'The information is not available in the provided context.'\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# Fill the template\n",
    "prompt = prompt_template.format(context=context_str, question=question)\n",
    "\n",
    "# Call Watsonx LLM\n",
    "response = llm(prompt)\n",
    "\n",
    "# Display the result\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f\"**Q:** {question}\\n\\n**A:** {response}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
