{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38184a03",
   "metadata": {},
   "source": [
    "### Local RAG Workflow with Db2 and llama.cpp\n",
    "\n",
    "This notebook demonstrates a complete Retrieval-Augmented Generation (RAG) pipeline running on a local system:\n",
    "\n",
    "* **Embeddings** are generated locally using the Granite model served via `llama.cpp`\n",
    "* **Vector search** is performed in Db2 using built-in `VECTOR` functions\n",
    "* **Context retrieval** and **prompt construction** are handled locally\n",
    "* Only the **final text generation** step uses a hosted LLM (Mistral) via Watsonx.ai\n",
    "\n",
    "By keeping embedding generation and prompting local, the workflow reduces latency, avoids cloud dependency for sensitive data, and offers more control over the overall process.\n",
    "\n",
    "**Setup and Requirements**\n",
    "For installation steps and additional context, see the accompanying [README.md](./README.md).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f5ff2",
   "metadata": {},
   "source": [
    "## Step 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8a40ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1708: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2305: SyntaxWarning: invalid escape sequence '\\?'\n",
      "/tmp/ipykernel_395841/2299624180.py:1708: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  firstCommand = \"(?:^\\s*)([a-zA-Z]+)(?:\\s+.*|$)\"\n",
      "/tmp/ipykernel_395841/2299624180.py:2305: SyntaxWarning: invalid escape sequence '\\?'\n",
      "  pattern = \"\\?\\*[0-9]+\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Db2 Extensions Loaded. Version: 2024-09-16\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "# NLP and embeddings\n",
    "import spacy\n",
    "import trafilatura\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Watsonx AI\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams, EmbedTextParamsMetaNames\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "# Notebook display utilities\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "if not os.path.isfile('db2.ipynb'):\n",
    "    os.system('wget https://raw.githubusercontent.com/IBM/db2-jupyter/master/db2.ipynb')\n",
    "\n",
    "%run db2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342720d9",
   "metadata": {},
   "source": [
    "## Step 1: Web Content Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4ef41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despite being one of the earlier machine learning techniques, linear regression continues to be a top choice among ML practitioners for a regression task. For the past three years, over 80% of the respondents to Kaggle’s annual state of data science and machine learning survey mentioned linear regression as a ML algorithm they most frequently use. IBM Db2 provides an in-database stored procedure (SP) for Linear Regression as part of its ML library, which is a collection of over 200 SPs for performing different ML tasks in the database. Using the linear regression SP and other functionality of DB2’s ML Library, ML practitioners can build and deploy linear regression models in the database when their ML dataset is available in a Db2 database. In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\n",
      "Let’s begin.\n",
      "The Regression Task\n",
      "In this exercise, I will use the GoSales dataset, which is available from this link.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://community.ibm.com/community/user/blogs/shaikh-quader/2024/05/07/building-an-in-db-linear-regression-model-with-ibm'\n",
    "downloaded = trafilatura.fetch_url(url)\n",
    "\n",
    "if downloaded:\n",
    "    article = trafilatura.extract(downloaded)\n",
    "    print(article[:1000])  # Preview first 1000 chars\n",
    "else:\n",
    "    print(\"Failed to fetch content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53037a08",
   "metadata": {},
   "source": [
    "## Step 2: Chunking the Clean Text for Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6353970",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76455699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_sentence_chunker(text, max_words=200, overlap_words=50):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        sentence = sentences[i]\n",
    "        sentence_length = len(sentence.split())\n",
    "\n",
    "        if current_length + sentence_length <= max_words:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "            i += 1\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            # Start new chunk with overlap\n",
    "            overlap = []\n",
    "            overlap_len = 0\n",
    "            j = len(current_chunk) - 1\n",
    "            while j >= 0 and overlap_len < overlap_words:\n",
    "                s = current_chunk[j]\n",
    "                overlap.insert(0, s)\n",
    "                overlap_len += len(s.split())\n",
    "                j -= 1\n",
    "            current_chunk = overlap\n",
    "            current_length = overlap_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757864ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 chunks created.\n",
      "Despite being one of the earlier machine learning techniques, linear regression continues to be a top choice among ML practitioners for a regression task. For the past three years, over 80% of the respondents to Kaggle’s annual state of data science and machine learning survey mentioned linear regression as a ML algorithm they most frequently use. IBM Db2 provides an in-database stored procedure (SP) for Linear Regression as part of its ML library, which is a collection of over 200 SPs for performing different ML tasks in the database. Using the linear regression SP and other functionality of DB2’s ML Library, ML practitioners can build and deploy linear regression models in the database when their ML dataset is available in a Db2 database. In this post, I will show you the following steps of building and using a linear regression pipeline using SQL with a Db2 database:\n",
      "Let’s begin. The Regression Task\n",
      "In this exercise, I will use the GoSales dataset, which is available from this link. The dataset has 60252 synthetic customers’ profile and their purchase amount at an imaginary outdoor equipment store.\n"
     ]
    }
   ],
   "source": [
    "chunks = overlapping_sentence_chunker(article, max_words=200, overlap_words=50)\n",
    "print(f\"{len(chunks)} chunks created.\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270abedb",
   "metadata": {},
   "source": [
    "## Generating Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5823e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Llama(model_path=\"models/granite-embedding-30m-english-Q6_K.gguf\", embedding=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a43b6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_model.create_embedding(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e010689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text, item in zip(chunks, embeddings[\"data\"]):\n",
    "#     print(f\"Text: {text}\")\n",
    "#     print(\"Embedding (partial):\", item[\"embedding\"][:12], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d707a81",
   "metadata": {},
   "source": [
    "## Inserting Vectors into Db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea52ddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful. SAMPLE @ localhost \n",
      "Command completed.\n",
      "Command completed.\n"
     ]
    }
   ],
   "source": [
    "db2creds = dotenv_values('.env')\n",
    "%sql CONNECT CREDENTIALS db2creds\n",
    "\n",
    "%sql DROP TABLE IF EXISTS embeddings\n",
    "\n",
    "SQL_CREATE_TABLE = \"\"\"\n",
    "CREATE TABLE embeddings (\n",
    "    id INT NOT NULL GENERATED ALWAYS AS IDENTITY \n",
    "        (START WITH 1, INCREMENT BY 1),\n",
    "    content CLOB,\n",
    "    embedding VECTOR(384, FLOAT32),\n",
    "    PRIMARY KEY (id)\n",
    ")\n",
    "\"\"\"\n",
    "%sql {SQL_CREATE_TABLE}\n",
    "\n",
    "# Prepare values: list of tuples (content, embedding_vector_str)\n",
    "values = []\n",
    "for i, (text, item) in enumerate(zip(chunks, embeddings[\"data\"])):\n",
    "    embedding = item.get(\"embedding\")\n",
    "    if embedding and len(embedding) == 384:\n",
    "        vector_str = \"[\" + \", \".join(map(str, embedding)) + \"]\"\n",
    "        values.append((text, vector_str))\n",
    "    else:\n",
    "        print(f\"Skipping row {i+1}: invalid embedding\")\n",
    "\n",
    "# Prepare SQL statement with VECTOR function\n",
    "stmt = %sql prepare INSERT INTO embeddings (content, embedding) VALUES (?, VECTOR(?, 384, FLOAT32))\n",
    "\n",
    "# Disable autocommit\n",
    "%sql autocommit off\n",
    "\n",
    "# Execute prepared insert statement using magic-style loop\n",
    "for content, vector_str in values:\n",
    "    %sql execute :stmt using :content, :vector_str\n",
    "\n",
    "# Commit the work\n",
    "%sql commit work\n",
    "\n",
    "# Enable autocommit back\n",
    "%sql autocommit on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b2000",
   "metadata": {},
   "source": [
    "# Ask LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ddc4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to build a linear regression model using IDAX?\"\n",
    "# question = 'How to see the list of in database ML models in Db2?'\n",
    "# question = 'How to impute missing values of columns in Db2?'\n",
    "# query = 'What is Python UDF?'\n",
    "\n",
    "embedding = embedding_model.create_embedding(question)\n",
    "query_vector = embedding['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b31b2",
   "metadata": {},
   "source": [
    "## Vector Search and Context Retrieval (Db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d642f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Format query embedding for Db2\n",
    "query_embedding_str = '[' + ', '.join(map(str, query_vector)) + ']'\n",
    "top_k = 5  # You can adjust this number\n",
    "\n",
    "# Step 2: SQL to get top-k most similar chunks based on Euclidean distance\n",
    "SQL_DISTANCE = f\"\"\"\n",
    "SELECT \n",
    "    content AS CONTEXT,\n",
    "    VECTOR_DISTANCE(\n",
    "        VECTOR('{query_embedding_str}', 384, FLOAT32),\n",
    "        embedding,\n",
    "        EUCLIDEAN\n",
    "    ) AS DISTANCE\n",
    "FROM embeddings\n",
    "ORDER BY DISTANCE ASC\n",
    "FETCH FIRST {top_k} ROWS ONLY\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Run the query with db2magic and store the result in a DataFrame\n",
    "result_df = %sql {SQL_DISTANCE}\n",
    "\n",
    "# Step 4: Combine all top-k text chunks into a single context string\n",
    "context_str = \"\\n\\n\".join(result_df[\"CONTEXT\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826dc32",
   "metadata": {},
   "source": [
    "## LLM setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd12400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "load_dotenv(os.getcwd()+\"/.env\", override=True)\n",
    "\n",
    "# llm for generating responses\n",
    "def get_llm():\n",
    "    # model_id = 'meta-llama/llama-3-1-70b-instruct'\n",
    "    model_id = 'mistralai/mistral-large'\n",
    "    parameters = {\n",
    "        GenParams.MAX_NEW_TOKENS: 512,\n",
    "        GenParams.TEMPERATURE: 0.6,\n",
    "    }\n",
    "   \n",
    "    watsonx_llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        apikey=os.getenv(\"WATSONX_APIKEY\", \"\"),\n",
    "        project_id=os.getenv(\"WATSONX_PROJECT\", \"\"),\n",
    "        params=parameters,\n",
    "    )\n",
    "    return watsonx_llm\n",
    "    \n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d5da3",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bc8beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a knowledgeable assistant. Answer the question based solely on the provided context.\n",
    "- If the context contains the answer, respond directly to the reader using 'you' to make it personal.\n",
    "- If the answer includes code, provide an explanation of the code following the code block.\n",
    "- If the information is not available in the context, respond with 'The information is not available in the provided context.'\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1add39",
   "metadata": {},
   "source": [
    "### Generate an Answer Using wx.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "910d27e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Q:** How to build a linear regression model using IDAX?\n",
       "\n",
       "**A:** \n",
       "To build a linear regression model using IDAX, you can use the `LINEAR_REGRESSION` stored procedure (SP). Here is an example of how to call this SP:\n",
       "\n",
       "```sql\n",
       "CALL IDAX.LINEAR_REGRESSION('model=GOSALES.GOSALES_LINREG, intable=GOSALES.GOSALES_TRAIN, id=ID, target=PURCHASE_AMOUNT,incolumn=AGE;GENDER;MARITAL_STATUS;PROFESSION, intercept=true');\n",
       "```\n",
       "\n",
       "### Explanation:\n",
       "- `model=GOSALES.GOSALES_LINREG`: Specifies the name and schema for the model.\n",
       "- `intable=GOSALES.GOSALES_TRAIN`: Specifies the input table containing the training data.\n",
       "- `id=ID`: Specifies the unique identifier column in the training data.\n",
       "- `target=PURCHASE_AMOUNT`: Specifies the target column to predict.\n",
       "- `incolumn=AGE;GENDER;MARITAL_STATUS;PROFESSION`: Specifies the input feature columns.\n",
       "- `intercept=true`: Indicates that the model should include an intercept term.\n",
       "\n",
       "After the training completes, the new model, `GOSALES_LINREG`, will be added to Db2’s model catalog. You can list the existing models in the catalog using:\n",
       "\n",
       "```sql\n",
       "CALL IDAX.LIST_MODELS('format=short, all=true');\n",
       "```\n",
       "\n",
       "Additionally, the learned values of the intercept and coefficients will be saved in a metadata table named `GOSALES_LINREG_MODEL`. You can display these values using:\n",
       "\n",
       "```sql\n",
       "SELECT VAR_NAME, LEVEL_NAME, VALUE FROM GOSALES.GOSALES_LINREG_MODEL;\n",
       "```\n",
       "\n",
       "This will show the learned parameters of your linear regression model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Custom prompt template (no LangChain needed)\n",
    "prompt_template = \"\"\"\n",
    "You are a knowledgeable assistant. Answer the question based solely on the provided context.\n",
    "- If the context contains the answer, respond directly to the reader using 'you' to make it personal.\n",
    "- If the answer includes code, provide an explanation of the code following the code block.\n",
    "- If the information is not available in the context, respond with 'The information is not available in the provided context.'\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "# Fill the template\n",
    "prompt = prompt_template.format(context=context_str, question=question)\n",
    "\n",
    "# Call Watsonx LLM\n",
    "response = llm(prompt)\n",
    "\n",
    "# Display the result\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f\"**Q:** {question}\\n\\n**A:** {response}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
